{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2b64cdc-e592-4b7a-a216-2f72c86514e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7555ea4c-dc27-473a-8ce5-37d8eedd696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prompt-guide.txt\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "270b9334-1991-4d8f-b977-cf8b742fbc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt engineering\n",
      "This guide shares strategies and tactics for getting better results from large language models (sometimes referred to as GPT models) like GPT-4. The methods described here can sometimes be deployed in combination for greater effect. We encourage experimentation to find the methods that work best for you.\n",
      "\n",
      "Some of the examples demonstrated here currently work only with our most capable model, gpt-4. In general, if you find that a model fails at a task and a more capable model i\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4749a96-6a0f-49a5-8a45-ac53d856d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aacf1e4-84d2-48aa-836e-f0b8f33a2e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"{text}\n",
    "----------------------------\n",
    "Based on above intructions, help me write a good prompt TEMPLATE.\n",
    "\n",
    "This template should be python f-string. It can take in any number of variables depending on my objectives.\n",
    "\n",
    "Return your answer in the following format:\n",
    "\n",
    "```prompt\n",
    "...\n",
    "```\n",
    "\n",
    "This is my objective:\n",
    "\n",
    "{objective}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62404c9b-5fb7-4831-8085-d80c35d9399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt = prompt.partial(text = text)\n",
    "chain = prompt | ChatOpenAI(model=\"gpt-4-1106-preview\") | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e505144b-e185-4c0d-b4e4-d64af796005d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```prompt\\nSYSTEM\\nUse the provided reference text delimited by triple quotes to answer the question. If the answer cannot be found in the reference text, write \"The information is not available based on the provided context.\"\\n\\nUSER\\n\"\"\"{context}\"\"\"\\n\\nQuestion: {question}\\n```\\n\\nIn this template, `{context}` will be the variable holding the reference text you want the model to use for answering the question, and `{question}` will be the variable holding the question you want answered. This prompt template instructs the model to rely solely on the provided text when answering the question and provides a clear protocol for indicating when the answer is not contained within the provided context.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = \"Answer a question based on context provided, and ONLY on that context.\"\n",
    "chain.invoke({\"objective\": task})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9cccb50-568f-4124-88ce-781c665c385d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on your objective, the prompt template should encourage the language model to focus on the provided context to generate an accurate response to a question. Here's an example of a Python f-string template that you can use for this purpose:\n",
      "\n",
      "```python\n",
      "prompt = f\"\"\"\n",
      "Given the context provided below, answer the following question using only the information from the context. If the information needed to answer the question is not available in the context, please respond with \"The information provided is insufficient to answer the question.\"\n",
      "\n",
      "Context:\n",
      "{{context}}\n",
      "\n",
      "Question:\n",
      "{{question}}\n",
      "\"\"\"\n",
      "\n",
      "context = \"\"\"Your context goes here...\"\"\"  # Replace with your actual context\n",
      "question = \"Your question goes here...\"  # Replace with your actual question\n",
      "\n",
      "# You would use the template like this\n",
      "full_prompt = prompt.format(context=context, question=question)\n",
      "```\n",
      "\n",
      "When you use `full_prompt`, it will contain the instructions and placeholders filled with the actual `context` and `question` you want to ask. The model will then generate an answer based on the context provided."
     ]
    }
   ],
   "source": [
    "task = \"Answer a question based on context provided, and ONLY on that context.\"\n",
    "for token in chain.stream({\"objective\": task}):\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3cc20bc-2109-421f-b7c9-caec153089c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"{text}\n",
    "----------------------------\n",
    "\n",
    "----------------------------\n",
    "\n",
    "----------------------------\n",
    "Based on above intructions, help me write a good prompt TEMPLATE.\n",
    "\n",
    "This template should be a string that can be formatted as python f-string. It can take in any number of variables depending on my objectives.\n",
    "\n",
    "Return your answer in the following format:\n",
    "\n",
    "```prompt\n",
    "...\n",
    "```\n",
    "\n",
    "This is my objective:\n",
    "\n",
    "{objective}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dd33a58-d1e6-4d7c-8d12-25af3e25b72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt = prompt.partial(text = text)\n",
    "chain = prompt | ChatOpenAI(model=\"gpt-4-1106-preview\") | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "842a0342-9987-4b44-b366-d803f4c4038f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```prompt\n",
      "prompt = f\"\"\"\n",
      "Please provide an answer to the following question based solely on the context provided below. If the information within the context is insufficient to provide a full answer, respond with 'The provided context does not contain enough information to answer the question.'\n",
      "\n",
      "Context:\n",
      "{{context}}\n",
      "\n",
      "Question:\n",
      "{{question}}\n",
      "\"\"\"\n",
      "```\n",
      "\n",
      "In this template, you will replace `{{context}}` with the actual text of the context you're providing and `{{question}}` with the question you want the model to answer based on that context. Here's an example of how you might use this template in practice:\n",
      "\n",
      "```python\n",
      "context = \"The industrial revolution marked a major turning point in history; almost every aspect of daily life was influenced in some way. Average income and population began to exhibit unprecedented sustained growth. In the two centuries following 1800, the world's average per capita income increased over tenfold, while the world's population increased over sixfold.\"\n",
      "\n",
      "question = \"What were the effects of the industrial revolution on average income and population growth?\"\n",
      "\n",
      "formatted_prompt = prompt.format(context=context, question=question)\n",
      "\n",
      "print(formatted_prompt)\n",
      "```"
     ]
    }
   ],
   "source": [
    "task = \"Answer a question based on context provided, and ONLY on that context.\"\n",
    "for token in chain.stream({\"objective\": task}):\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5228ca86-44e2-4aa2-b1ad-32e8b4b86d72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
